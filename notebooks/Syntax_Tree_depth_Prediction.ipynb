{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "name": "Syntax Tree-depth Prediction.ipynb",
            "provenance": [],
            "collapsed_sections": [],
        },
        "kernelspec": {"display_name": "Python 3", "name": "python3"},
    },
    "cells": [
        {
            "cell_type": "code",
            "metadata": {"id": "A1SouZXWbOPb"},
            "source": [
                "#INSTALL REQUIREMENTS\n",
                "!pip install transformers==3.5.1\n",
                "!pip install pyconll\n",
                "!pip install pkbar\n",
                "!pip install torch==1.4.0",
            ],
            "execution_count": null,
            "outputs": [],
        },
        {
            "cell_type": "code",
            "metadata": {"id": "w9s_coERT5vZ"},
            "source": [
                "# IMPORTS\n",
                "from io import open\n",
                "import pathlib\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import re\n",
                "import os\n",
                "import random\n",
                "import time\n",
                "import pyconll\n",
                "import pkbar\n",
                "\n",
                "import torch\n",
                "from torch import nn\n",
                "from torch.nn.functional import softmax\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from torch.utils.tensorboard import SummaryWriter\n",
                "%load_ext tensorboard\n",
                "\n",
                "import transformers\n",
                "from transformers import BertForSequenceClassification, AdamW, BertModel, BertConfig, AlbertForSequenceClassification\n",
                "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
                "\n",
                "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
                "\n",
                "import matplotlib.pyplot as plt\n",
                "%matplotlib inline\n",
                "\n",
                "# SEEDING\n",
                "seed_val = 42\n",
                "random.seed(seed_val)\n",
                "np.random.seed(seed_val)\n",
                "torch.manual_seed(seed_val)\n",
                "torch.cuda.manual_seed_all(seed_val)\n",
                "\n",
                "torch.backends.cudnn.deterministic = True\n",
                "torch.backends.cudnn.benchmark = False\n",
                "\n",
                "os.environ['PYTHONHASHSEED'] = str(seed_val)\n",
                "\n",
                "# DEVICE\n",
                "device = ('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
                'print("using ", device)',
            ],
            "execution_count": null,
            "outputs": [],
        },
        {
            "cell_type": "markdown",
            "metadata": {"id": "jDY6mHkYASLb"},
            "source": ["### HYPERPARAMS"],
        },
        {
            "cell_type": "code",
            "metadata": {"id": "AOXZYKBRAU9p"},
            "source": [
                "# MODEL HYPERPARAMS\n",
                "LAYER_NO = 12\n",
                "DROPOUT = 0.1\n",
                "MODEL_NAME = 'ai4bharat/indic-bert'\n",
                "\n",
                "# OPTIMIZATION HYPERPARAMS\n",
                "LEARNING_RATE = 1e-2\n",
                "BATCH_SIZE = 128\n",
                "EPOCHS = 30\n",
                "PATIENCE = 2\n",
                "WEIGHT_DECAY = 1e-6",
            ],
            "execution_count": null,
            "outputs": [],
        },
        {
            "cell_type": "code",
            "metadata": {"id": "tBjxFY8B6wSA"},
            "source": [
                "# PATH\n",
                'TRAIN_PATH = "train.conllu" \n',
                'TEST_PATH = "test.conllu"\n',
                'DEV_PATH = "dev.conllu"\n',
                "\n",
                'SAVE_PATH = "checkpoints"\n',
                "if not os.path.exists(SAVE_PATH):\n",
                "    os.mkdir(SAVE_PATH)\n",
                "    os.mkdir(SAVE_PATH / MODEL_NAME)\n",
                "\n",
                'experiment_id = "cgi_tam_tl_all_trans_{}_{}_{}".format("tree", "indic-bert", LAYER_NO)',
            ],
            "execution_count": null,
            "outputs": [],
        },
        {
            "cell_type": "code",
            "metadata": {"id": "ImYkmwInI01V"},
            "source": [
                "# HELPER FUNCTIONS\n",
                "def recur(s):\n",
                "    if(len(s) == 0):\n",
                "      return 1\n",
                "    ans = 0\n",
                "    for c in s:\n",
                "      ans = max(ans, recur(c))\n",
                "    ans = ans + 1 \n",
                "    return ans\n",
                "\n",
                "def treedepth(sentence):\n",
                "  s = sentence.to_tree()\n",
                "  depth = recur(s)\n",
                "  return depth-1",
            ],
            "execution_count": null,
            "outputs": [],
        },
        {
            "cell_type": "code",
            "metadata": {"id": "gP3DaIYNHzdK"},
            "source": [
                "# GENERATE LISTS OF SENTENCES AND CORRESPONDING DEPTHS\n",
                "def generate_data(dataset_path):\n",
                "  #returns a sentence and its tree-depth\n",
                "  dataset = pyconll.load_from_file(dataset_path)\n",
                "  sentences = []\n",
                "  depths = []\n",
                "\n",
                "  #change s to token/text depending on sequence classification class\n",
                "  #currently s is a sentence object\n",
                "  for s in dataset:\n",
                "    inter = []\n",
                "    for t in s:\n",
                "      inter.append(t.form) \n",
                "    depths.append(treedepth(s))\n",
                '    sentences.append(" ".join(inter))\n',
                "  return sentences, depths",
            ],
            "execution_count": null,
            "outputs": [],
        },
        {
            "cell_type": "code",
            "metadata": {"id": "1T2XmnS0z7ap"},
            "source": [
                "# EXTRACT SENTENCE AND DEPTH\n",
                "train_texts, train_depth = generate_data(TRAIN_PATH)\n",
                "val_texts, val_depth = generate_data(DEV_PATH)\n",
                "test_texts, test_depth = generate_data(TEST_PATH)",
            ],
            "execution_count": null,
            "outputs": [],
        },
        {
            "cell_type": "code",
            "metadata": {"id": "-SA1wEBQ3N0S"},
            "source": [
                "# GET THE UNIQUE DEPTHS AVAILABLE IN THE TOTAL DATASET\n",
                "total_depth = train_depth + val_depth + test_depth\n",
                "total_depth_unique = set(total_depth)\n",
                "total_depth_unique = {0,1,2,3,4,5,6,7,8,9,10,11,12,13,14}\n",
                "unique_depths = len(total_depth_unique)\n",
                "print(total_depth_unique)\n",
                "print(unique_depths)",
            ],
            "execution_count": null,
            "outputs": [],
        },
        {
            "cell_type": "code",
            "metadata": {"id": "KxtuJ_ist4jg"},
            "source": [
                "# LINEAR PROBING MODEL\n",
                "class SequenceClassifierProbe(AlbertForSequenceClassification):\n",
                "    def __init__(self, config, LAYER_ID=12, DROPOUT=0.0):\n",
                "        super().__init__(config)\n",
                "        \n",
                "        # layer to extract output from, can be any integer in range [1,12]\n",
                "        self.layer_id = LAYER_ID\n",
                "        self.num_labels = config.num_labels\n",
                "\n",
                "        # enable outputing hidden states and freeze BertModel\n",
                "        self.lm = AutoModel.from_pretrained(MODEL_NAME, config=config)\n",
                "        for parameter in self.lm.parameters():\n",
                "            parameter.requires_grad = False\n",
                "\n",
                "        # linear probe\n",
                "        self.dropout = nn.Dropout(DROPOUT)\n",
                "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
                "\n",
                "    def forward(\n",
                "        self,\n",
                "        input_ids=None,\n",
                "        attention_mask=None,\n",
                "        token_type_ids=None,\n",
                "        position_ids=None,\n",
                "        head_mask=None,\n",
                "        inputs_embeds=None,\n",
                "        labels=None,\n",
                "        output_attentions=None,\n",
                "        output_hidden_states=None,\n",
                "        return_dict=None,\n",
                "    ):\n",
                '        """\n',
                "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
                "            Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[0, ...,\n",
                "            config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n",
                "            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
                '        """\n',
                "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
                "        outputs = self.lm(\n",
                "            input_ids,\n",
                "            attention_mask=attention_mask,\n",
                "            token_type_ids=token_type_ids,\n",
                "            position_ids=position_ids,\n",
                "            head_mask=head_mask,\n",
                "            inputs_embeds=inputs_embeds,\n",
                "            output_attentions=output_attentions,\n",
                "            output_hidden_states=output_hidden_states,\n",
                "            return_dict=return_dict,\n",
                "        )\n",
                "                \n",
                "\n",
                "        # sequence_output = outputs[0]\n",
                "        # pooled_output = outputs[1]\n",
                "        hidden_states = outputs[2]\n",
                "        # -1 for 12th layer, -2 for 11th layer and so on\n",
                "        #h = hidden_states[layer_id - 13][:, 0].reshape((-1, 1, 768)) \n",
                "        h = torch.squeeze(hidden_states[self.layer_id][:, 0, :])\n",
                "        pooled_output = self.dropout(h)\n",
                "        logits = self.classifier(pooled_output)\n",
                "        loss = None\n",
                "\n",
                "        if labels is not None:\n",
                "            if self.num_labels == 1:\n",
                "                #  We are doing regression\n",
                "                loss_fct = MSELoss()\n",
                "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
                "            else:\n",
                "                loss_fct = nn.CrossEntropyLoss()\n",
                "                # print(logits.shape)\n",
                "                a = logits.view(-1, self.num_labels)\n",
                "                # print(a.shape)\n",
                "                b = labels.view(-1)\n",
                "                # print(b.shape)\n",
                "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
                "\n",
                "        if not return_dict:\n",
                "            output = (logits,) + outputs[2:]\n",
                "            return ((loss,) + output) if loss is not None else output\n",
                "\n",
                "        return SequenceClassifierOutput(\n",
                "            loss=loss,\n",
                "            logits=logits,\n",
                "            hidden_states=outputs.hidden_states,\n",
                "            attentions=outputs.attentions,\n",
                "        )",
            ],
            "execution_count": null,
            "outputs": [],
        },
        {
            "cell_type": "code",
            "metadata": {"id": "U2TXU2Ubyh7T"},
            "source": [
                "# TOKENIZER\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
                "\n",
                "# PRE-TRAINED MODEL and PROBE\n",
                "config = AutoConfig.from_pretrained(MODEL_NAME, output_hidden_states=True, num_labels=len(total_depth_unique), max_position_embeddings=512)\n",
                "model = SequenceClassifierProbe(config, LAYER_ID=LAYER_NO, DROPOUT=DROPOUT).to(device)",
            ],
            "execution_count": null,
            "outputs": [],
        },
        {
            "cell_type": "code",
            "metadata": {"id": "0jbW77MK6MN8"},
            "source": [
                "# TOKENIZE DATA\n",
                "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
                "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
                "test_encodings = tokenizer(test_texts, truncation=True, padding=True)",
            ],
            "execution_count": null,
            "outputs": [],
        },
        {
            "cell_type": "code",
            "metadata": {"id": "La4UhcGH7-sh"},
            "source": [
                "# DATASETS\n",
                "class TreeDepthDataset(Dataset):\n",
                "    def __init__(self, encodings, labels):\n",
                "        self.encodings = encodings\n",
                "        self.labels = labels\n",
                "        \n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
                "        item['labels'] = torch.tensor(self.labels[idx])\n",
                "        return item\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.labels)\n",
                "\n",
                "train_dataset = TreeDepthDataset(train_encodings, train_depth)\n",
                "val_dataset = TreeDepthDataset(val_encodings, val_depth)\n",
                "test_dataset = TreeDepthDataset(test_encodings, test_depth)",
            ],
            "execution_count": null,
            "outputs": [],
        },
        {
            "cell_type": "code",
            "metadata": {"id": "4DQjfO2V7plN"},
            "source": [
                "# DATALOADERS\n",
                "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
                "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
                "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
                "\n",
                "optim = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)",
            ],
            "execution_count": null,
            "outputs": [],
        },
        {
            "cell_type": "code",
            "metadata": {"id": "V2siXTqUVB_r"},
            "source": [
                "# TENSORBOARD LOGGER\n",
                "writer = SummaryWriter(log_dir=str(SAVE_PATH / MODEL_NAME), comment=experiment_id)",
            ],
            "execution_count": null,
            "outputs": [],
        },
        {
            "cell_type": "code",
            "metadata": {"id": "QEP9EYLzVC5N"},
            "source": [
                "# TRAIN PROBE\n",
                "min_val_loss = 999999\n",
                "checkpoint_epoch = 0\n",
                "COUNTER = 0\n",
                "\n",
                "for epoch in range(EPOCHS):\n",
                "\n",
                "    # progress bar\n",
                '    print("\\nTraining Epoch: %d/%d" % (epoch + 1, EPOCHS))\n',
                "    kbar = pkbar.Kbar(target=len(train_loader), width=100)\n",
                "\n",
                "    model.train()\n",
                "    epoch_loss = 0.0\n",
                "    for batch_id, batch_data in enumerate(train_loader):\n",
                "        # forward pass\n",
                "        optim.zero_grad()\n",
                "        input_ids = batch_data['input_ids'].to(device)\n",
                "        attention_mask = batch_data['attention_mask'].to(device)\n",
                "        # token_type_ids = batch['token_type_ids'].to(device)\n",
                "        labels = batch_data['labels'].to(device)\n",
                "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
                "\n",
                "        # backward pass\n",
                "        loss = outputs[0]\n",
                "        loss.backward()\n",
                "        optim.step()\n",
                "\n",
                "        # progress & logging\n",
                "        epoch_loss += loss.item()\n",
                '        kbar.update(batch_id, values=[("Training Loss", loss.item())])\n',
                '        writer.add_scalar("Running Loss", loss.item(), epoch * len(train_loader) + batch_id)\n',
                "\n",
                "    # log epoch training loss\n",
                '    writer.add_scalar("Epoch Train Loss", epoch_loss/len(train_dataset), epoch+1)\n',
                "    \n",
                "    # validation\n",
                "    val_loss = 0.0\n",
                "    model.eval()\n",
                "\n",
                "    # progress bar\n",
                '    print("\\nValidating Epoch: %d/%d" % (epoch + 1, EPOCHS))\n',
                "    kbar = pkbar.Kbar(target=len(val_loader), width=64)\n",
                "\n",
                "    with torch.no_grad():\n",
                "      for batch_id, batch_data in enumerate(val_loader):\n",
                "        input_ids = batch_data['input_ids'].to(device)\n",
                "        attention_mask = batch_data['attention_mask'].to(device)\n",
                "        # token_type_ids = batch['token_type_ids'].to(device)\n",
                "        labels = batch_data['labels'].to(device)\n",
                "\n",
                "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
                "        loss = outputs[0]\n",
                "\n",
                "        # progress & logging\n",
                "        val_loss += loss.item()\n",
                '        kbar.update(batch_id, values=[("Validation Loss", loss.item())])\n',
                "\n",
                "    # log epoch validation loss\n",
                '    writer.add_scalar("Epoch Valid Loss", val_loss/len(val_dataset), epoch+1)\n',
                "\n",
                "    if ((val_loss/len(val_dataset))<min_val_loss):\n",
                "        print(\"\\nModel Optimized! Saving Weights...\", '\\n')\n",
                "        min_val_loss = (val_loss/len(val_dataset))\n",
                "        checkpoint_epoch = epoch + 1\n",
                "        torch.save(model.classifier.state_dict(), SAVE_PATH / MODEL_NAME / (experiment_id + '.pt'))\n",
                "        COUNTER = 0\n",
                "    else:\n",
                "        COUNTER += 1\n",
                "        if COUNTER >= PATIENCE:\n",
                "          print('\\nEarly stopping!\\n')\n",
                "          break",
            ],
            "execution_count": null,
            "outputs": [],
        },
        {
            "cell_type": "code",
            "metadata": {"id": "HeIJ71OlVHAo"},
            "source": [
                "# PREDICTIONS ON TEST \n",
                "preds = []\n",
                "truth = []\n",
                "model.classifier.load_state_dict(torch.load(SAVE_PATH / MODEL_NAME / (experiment_id + '.pt')))\n",
                "model.eval()\n",
                "\n",
                "with torch.no_grad():\n",
                "    for batch_id, batch_data in enumerate(test_loader):\n",
                "        labels_index = []\n",
                "        input_ids = batch_data['input_ids'].to(device)\n",
                "        attention_mask = batch_data['attention_mask'].to(device)\n",
                "        # token_type_ids = batch['token_type_ids'].to(device)\n",
                "        labels = batch_data['labels'].to(device)\n",
                "      \n",
                "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)        \n",
                "        logits = outputs[1]\n",
                "        soft = softmax(logits, dim=1)\n",
                "        _, predictions = torch.max(soft, dim=1, keepdim=False)\n",
                "        for i in range(len(predictions)):\n",
                "          preds.append(predictions[i].item())\n",
                "        for label in labels:\n",
                "          truth.append(label.item())",
            ],
            "execution_count": null,
            "outputs": [],
        },
        {
            "cell_type": "code",
            "metadata": {"id": "FYCVgZrlVLdm"},
            "source": [
                "# USE PREDS LIST AND GROUND TRUTH LIST TO GET ACC & F1\n",
                "acc = accuracy_score(truth, preds)\n",
                "prec, recall, f1, _ = precision_recall_fscore_support(truth, preds, average='weighted')\n",
                'print("Accuracy: {}, F1: {}, Prec: {}, Recall: {}".format(acc, f1, prec, recall))',
            ],
            "execution_count": null,
            "outputs": [],
        },
        {
            "cell_type": "code",
            "metadata": {"id": "EliLbsoQVOkK"},
            "source": [
                "# PRINT METRICS\n",
                'print("accuracy", round(acc,4))\n',
                'print("F1", round(f1,4))\n',
                'print("Precision", round(prec,4))\n',
                'print("recall", round(recall,4))',
            ],
            "execution_count": null,
            "outputs": [],
        },
        {
            "cell_type": "code",
            "metadata": {"id": "CAX4PI1cVSEO"},
            "source": [
                "# CLOSING LOGS\n",
                "writer.add_hparams(\n",
                "    {\n",
                '        "model": MODEL_NAME,\n',
                '        "layer": LAYER_NO,\n',
                '        "bs": BATCH_SIZE,\n',
                '        "n_epochs": EPOCHS,\n',
                '        "checkpoint_epoch": checkpoint_epoch,\n',
                '        "lr": LEARNING_RATE,\n',
                '        "dropout": DROPOUT,\n',
                '        "patience": PATIENCE,\n',
                '        "weight_decay": WEIGHT_DECAY,\n',
                "    },\n",
                '    {"validation_loss": min_val_loss, "test_acc": acc, "test_f1": f1,  "test_prec": prec,  "test_recall": recall},\n',
                ")\n",
                "writer.flush()",
            ],
            "execution_count": null,
            "outputs": [],
        },
        {
            "cell_type": "code",
            "metadata": {"id": "hOgFaRRqVYMN"},
            "source": [
                "# VISUALIZE LOGS\n",
                "print(SAVE_PATH / MODEL_NAME)\n",
                '%tensorboard --logdir "checkpoints/ai4bharat/indic-bert"',
            ],
            "execution_count": null,
            "outputs": [],
        },
    ],
}
