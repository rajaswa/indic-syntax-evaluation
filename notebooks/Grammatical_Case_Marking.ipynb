{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "name": "Grammatical Case Marking.ipynb",
            "provenance": [],
            "collapsed_sections": [],
            "machine_shape": "hm",
        },
        "kernelspec": {"display_name": "Python 3", "name": "python3"},
    },
    "cells": [
        {
            "cell_type": "code",
            "metadata": {"id": "A1SouZXWbOPb"},
            "source": [
                "#INSTALL REQUIREMENTS\n",
                "!pip install transformers==3.5.1\n",
                "!pip install pyconll\n",
                "!pip install pkbar\n",
                "!pip3 install sentencepiece\n",
                "!pip install torch==1.4.0",
            ],
            "execution_count": null,
            "outputs": [],
        },
        {
            "cell_type": "code",
            "metadata": {"id": "w9s_coERT5vZ"},
            "source": [
                "# IMPORTS\n",
                "from io import open\n",
                "import pathlib\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import re\n",
                "import os\n",
                "import random\n",
                "import time\n",
                "import pyconll\n",
                "import pkbar\n",
                "\n",
                "import torch\n",
                "from torch import nn\n",
                "from torch.nn.functional import softmax\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from torch.utils.tensorboard import SummaryWriter\n",
                "%load_ext tensorboard\n",
                "\n",
                "import transformers\n",
                "from transformers import BertForTokenClassification, AdamW, BertModel, BertConfig, AlbertForTokenClassification\n",
                "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
                "\n",
                "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
                "\n",
                "import matplotlib.pyplot as plt\n",
                "%matplotlib inline\n",
                "\n",
                "# SEEDING\n",
                "seed_val = 42\n",
                "np.random.seed(seed_val)\n",
                "torch.manual_seed(seed_val)\n",
                "torch.cuda.manual_seed_all(seed_val)\n",
                "\n",
                "# DEVICE\n",
                "device = ('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
                'print("using ", device)',
            ],
            "execution_count": null,
            "outputs": [],
        },
        {
            "cell_type": "markdown",
            "metadata": {"id": "jDY6mHkYASLb"},
            "source": ["### HYPERPARAMS"],
        },
        {
            "cell_type": "code",
            "metadata": {"id": "AOXZYKBRAU9p"},
            "source": [
                "# MODEL HYPERPARAMS\n",
                "LAYER_NO = 12\n",
                "DROPOUT = 0.1\n",
                "MODEL_NAME = 'ai4bharat/indic-bert'\n",
                "\n",
                "# OPTIMIZATION HYPERPARAMS\n",
                "LEARNING_RATE = 1e-2\n",
                "BATCH_SIZE = 128\n",
                "EPOCHS = 30\n",
                "PATIENCE = 2\n",
                "WEIGHT_DECAY = 1e-6",
            ],
            "execution_count": null,
            "outputs": [],
        },
        {
            "cell_type": "code",
            "metadata": {"id": "tBjxFY8B6wSA"},
            "source": [
                "# PATH\n",
                'TRAIN_PATH = "train.conllu" \n',
                'TEST_PATH = "test.conllu"\n',
                'DEV_PATH = "dev.conllu"\n',
                "\n",
                'SAVE_PATH = "checkpoints"\n',
                "if not os.path.exists(SAVE_PATH):\n",
                "    os.mkdir(SAVE_PATH)\n",
                "    os.mkdir(SAVE_PATH / MODEL_NAME)\n",
                "\n",
                'experiment_id = "{}_{}_{}".format("case", "indic-bert", LAYER_NO)',
            ],
            "execution_count": null,
            "outputs": [],
        },
        {
            "cell_type": "code",
            "metadata": {"id": "XuQXRNEFTOjj"},
            "source": [
                "# GENERATE LISTS OF TEXT TOKENS AND CORRESPONDING CASE TAGS \n",
                "def generate_data(dataset_path):\n",
                '  """\n',
                "  takes a conll file and returns token list and case list\n",
                '  """ \n',
                "  dataset = pyconll.load_from_file(dataset_path)\n",
                "\n",
                "  token_text = []\n",
                "  token_case = []\n",
                "\n",
                "  for sentence in dataset:\n",
                "\n",
                "    tokens = []\n",
                "    cases = []\n",
                "\n",
                "    for token in sentence:\n",
                "      if 'Case' in token.feats:\n",
                "        tokens.append(token.form)\n",
                "        cases.append(np.array(token.feats['Case']).astype(str))\n",
                "      else:\n",
                "        continue\n",
                "      \n",
                "      # break\n",
                "    token_text.append(tokens)\n",
                "    token_case.append(np.array(cases))\n",
                "    # break\n",
                "  token_case = np.array(token_case)\n",
                "  return token_text, token_case",
            ],
            "execution_count": null,
            "outputs": [],
        },
        {
            "cell_type": "code",
            "metadata": {"id": "1T2XmnS0z7ap"},
            "source": [
                "# GET CASE LABELS\n",
                "tokens_train, cases_train = generate_data(TRAIN_PATH)\n",
                "tokens_val, cases_val = generate_data(DEV_PATH)\n",
                "tokens_test, cases_test = generate_data(TEST_PATH)",
            ],
            "execution_count": null,
            "outputs": [],
        },
        {
            "cell_type": "code",
            "metadata": {"id": "8renjUbigFx2"},
            "source": [
                "# ENCODE CASE TO INTEGERS AND VICE-VERSA\n",
                "unique_cases = set(case for doc in cases_train for case in doc)\n",
                "case2id = {case: id for id, case in enumerate(unique_cases)}\n",
                "id2case = {id: case for case, id in case2id.items()}",
            ],
            "execution_count": null,
            "outputs": [],
        },
        {
            "cell_type": "code",
            "metadata": {"id": "lGeOS944nc_X"},
            "source": [
                "# LINEAR PROBING MODEL\n",
                "class TokenClassifierProbe(AlbertForTokenClassification):\n",
                "    def __init__(self, config, LAYER_ID=12, DROPOUT=0.0):\n",
                "        super().__init__(config)\n",
                "        \n",
                "        # layer to extract output from, can be any integer in range [1,12]\n",
                "        self.layer_id = LAYER_ID\n",
                "        self.num_labels = config.num_labels\n",
                "\n",
                "        # enable outputing hidden states and freeze BertModel\n",
                "        self.lm = AutoModel.from_pretrained(MODEL_NAME, config=config)\n",
                "        for parameter in self.lm.parameters():\n",
                "            parameter.requires_grad = False\n",
                "\n",
                "        # linear probe\n",
                "        self.dropout = nn.Dropout(DROPOUT)\n",
                "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
                "\n",
                "    def forward(\n",
                "        self,\n",
                "        input_ids=None,\n",
                "        attention_mask=None,\n",
                "        token_type_ids=None,\n",
                "        position_ids=None,\n",
                "        head_mask=None,\n",
                "        inputs_embeds=None,\n",
                "        labels=None,\n",
                "        output_attentions=None,\n",
                "        output_hidden_states=None,\n",
                "        return_dict=None,\n",
                "    ):\n",
                '        """\n',
                "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
                "            Labels for computing the token classification loss.\n",
                "            Indices should be in ``[0, ..., config.num_labels - 1]``.\n",
                '        """\n',
                "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
                "        outputs = self.lm(\n",
                "            input_ids,\n",
                "            attention_mask=attention_mask,\n",
                "            token_type_ids=token_type_ids,\n",
                "            position_ids=position_ids,\n",
                "            head_mask=head_mask,\n",
                "            inputs_embeds=inputs_embeds,\n",
                "            output_attentions=output_attentions,\n",
                "            output_hidden_states=output_hidden_states,\n",
                "            return_dict=return_dict,\n",
                "        )\n",
                "        # taking output from the specified layer\n",
                "        sequence_output = outputs[2][self.layer_id]\n",
                "        sequence_output = self.dropout(sequence_output)\n",
                "        logits = self.classifier(sequence_output)\n",
                "        # print(logits.shape)\n",
                "        loss = None\n",
                "        if labels is not None:\n",
                "            loss_fct = nn.CrossEntropyLoss()\n",
                "            # Only keep active parts of the loss\n",
                "            if attention_mask is not None:\n",
                "                active_loss = attention_mask.view(-1) == 1\n",
                "                active_logits = logits.view(-1, self.num_labels)\n",
                "                active_labels = torch.where(\n",
                "                    active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n",
                "                )\n",
                "                loss = loss_fct(active_logits, active_labels)\n",
                "            else:\n",
                "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
                "        if not return_dict:\n",
                "            output = (logits,) + outputs[2:]\n",
                "            return ((loss,) + output) if loss is not None else output\n",
                "        return TokenClassifierOutput(\n",
                "            loss=loss,\n",
                "            logits=logits,\n",
                "            hidden_states=outputs.hidden_states,\n",
                "            attentions=outputs.attentions,\n",
                "        )",
            ],
            "execution_count": null,
            "outputs": [],
        },
        {
            "cell_type": "code",
            "metadata": {"id": "wS6_x5TMNfLL"},
            "source": [
                "# TOKENIZER\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
                "\n",
                "# PRE-TRAINED MODEL and PROBE\n",
                "config = AutoConfig.from_pretrained(MODEL_NAME, output_hidden_states=True, num_labels=len(unique_cases), max_position_embeddings=512)\n",
                "model = TokenClassifierProbe(config, LAYER_ID=LAYER_NO, DROPOUT=DROPOUT).to(device)",
            ],
            "execution_count": null,
            "outputs": [],
        },
        {
            "cell_type": "code",
            "metadata": {"id": "J_Zsr9IDGzyJ"},
            "source": [
                "# TOKENIZE DATA\n",
                "# adding max_length = 128 for Indic-Bert (wasn't reqd in mBERT)\n",
                "train_encodings = tokenizer(tokens_train, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True, max_length = 128)\n",
                "val_encodings = tokenizer(tokens_val, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True, max_length = 128)\n",
                "test_encodings = tokenizer(tokens_test, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True, max_length = 128)",
            ],
            "execution_count": null,
            "outputs": [],
        },
        {
            "cell_type": "code",
            "metadata": {"id": "Fa6i2fqjaWRF"},
            "source": [
                "# CALCULATE LENGTH OF TRAIN, VAL, AND TEST DATASETS\n",
                "train_len = len(cases_train)\n",
                "val_len = len(cases_val)\n",
                "test_len = len(cases_test)",
            ],
            "execution_count": null,
            "outputs": [],
        },
        {
            "cell_type": "code",
            "metadata": {"id": "z36xcsNZP_nu"},
            "source": [
                "# CONVERT THE TOKENS TO THEIR RESPECTIVE INTEGER IDS\n",
                "def convert_token_to_id(encodings, length):\n",
                "  token_to_id = []\n",
                "  for i in range(length):\n",
                "    token_to_id.append(encodings[i].tokens)\n",
                "  return token_to_id\n",
                "\n",
                "train_token_ids = convert_token_to_id(train_encodings, train_len)\n",
                "val_token_ids = convert_token_to_id(val_encodings, val_len)\n",
                "test_token_ids = convert_token_to_id(test_encodings, test_len)",
            ],
            "execution_count": null,
            "outputs": [],
        },
        {
            "cell_type": "code",
            "metadata": {"id": "GJ_KUvT5V3XR"},
            "source": [
                "# # GET THE ID OF THE '_' TOKEN\n",
                "# print(train_token_ids[5][3])\n",
                "# print(tokenizer.convert_tokens_to_ids(train_token_ids[5][3])) # (Considered '8' as thats what the model is returning)\n",
                "# print(tokenizer.convert_tokens_to_ids('_')) # manually '_' gives different token id\n",
                "# print(tokenizer.convert_ids_to_tokens(5513))",
            ],
            "execution_count": null,
            "outputs": [],
        },
        {
            "cell_type": "code",
            "metadata": {"id": "CxQsPKTYKK8R"},
            "source": [
                "# ENCODE THE CASES IN A SPECIFIC MANNER\n",
                "def encode_cases(cases, encodings, tokenizer, token_id):\n",
                '  """\n',
                "  takes tokenizer encodings as input, \n",
                '  """\n',
                "  labels = [[case2id[case] for case in doc] for doc in cases] # convert pos tags to their integer ID\n",
                "  encoded_labels = []\n",
                "  labels_index = []\n",
                "  for doc_labels, doc_offset, doc_token_id in zip(labels, encodings.offset_mapping, token_id):\n",
                "    # create an empty array of -100\n",
                "    doc_enc_labels = np.ones(len(doc_offset), dtype=int) * -100 # array of length=number of word piece tokens in sentence\n",
                "    arr_offset = np.array(doc_offset) # convert doc_offset to numpy array\n",
                "    \n",
                "    # set labels whose first offset position is 0 (all other sub-words other than initial one) and the second is not 0 (CLS and PAD token)\n",
                "    a = (arr_offset[:,0] == 0) & (arr_offset[:,1] != 0) # true at required labels and false were the labels need to be ignored\n",
                "    \n",
                "    # print(doc_labels)\n",
                "    # print(doc_offset)\n",
                "    li = []\n",
                "    countl = 0 # count of doc_labels (original labels)\n",
                "    countel = 0 # count of doc_enc_labels (encoded labels) \n",
                "    for j in a: # iterate through every element of 'a'\n",
                "      if j:\n",
                "        if tokenizer.convert_tokens_to_ids(doc_token_id[countel]) != 8: # ignore the extra '_' token generated due to xlmr tokenizer\n",
                "          li.append(countel)\n",
                "          doc_enc_labels[countel] = doc_labels[countl] # if true append the label from doc_labels to the new required encoded labels list\n",
                "          countl = countl + 1 \n",
                "        countel = countel + 1 \n",
                "      else: # if false skip that label from doc_label and move to the next index at doc_enc_labels\n",
                "        countel = countel + 1\n",
                "\n",
                "    encoded_labels.append(doc_enc_labels.tolist())\n",
                "    labels_index.append(li)\n",
                "    \n",
                "  return encoded_labels, labels_index\n",
                "\n",
                "train_labels, train_labels_index = encode_cases(cases_train, train_encodings, tokenizer, train_token_ids)\n",
                "val_labels, val_labels_index = encode_cases(cases_val, val_encodings, tokenizer, val_token_ids)\n",
                "test_labels, test_labels_index = encode_cases(cases_test, test_encodings, tokenizer, test_token_ids)",
            ],
            "execution_count": null,
            "outputs": [],
        },
        {
            "cell_type": "code",
            "metadata": {"id": "g3PgrqeOj7nX"},
            "source": [
                "# DATASETS\n",
                "class CASEdataset(Dataset):\n",
                "    def __init__(self, encodings, labels):\n",
                "        self.encodings = encodings\n",
                "        self.labels = labels\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
                "        item['labels'] = torch.tensor(self.labels[idx])\n",
                "        return item\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.labels)\n",
                " \n",
                "train_dataset = CASEdataset(train_encodings, train_labels)\n",
                "val_dataset = CASEdataset(val_encodings, val_labels)\n",
                "test_dataset = CASEdataset(test_encodings, test_labels)",
            ],
            "execution_count": null,
            "outputs": [],
        },
        {
            "cell_type": "code",
            "metadata": {"id": "1bWAinOtz8KK"},
            "source": [
                "# DATALOADERS\n",
                "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
                "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
                "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
                "\n",
                "optim = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)",
            ],
            "execution_count": null,
            "outputs": [],
        },
        {
            "cell_type": "code",
            "metadata": {"id": "NbRl0CcbhAl4"},
            "source": [
                "# TENSORBOARD LOGGER\n",
                "writer = SummaryWriter(log_dir=str(SAVE_PATH / MODEL_NAME), comment=experiment_id)",
            ],
            "execution_count": null,
            "outputs": [],
        },
        {
            "cell_type": "code",
            "metadata": {"id": "Z_3gdSZMtiLY"},
            "source": [
                "# TRAIN PROBE\n",
                "min_val_loss = 999999\n",
                "checkpoint_epoch = 0\n",
                "COUNTER = 0\n",
                "\n",
                "for epoch in range(EPOCHS):\n",
                "\n",
                "    # progress bar\n",
                '    print("\\nTraining Epoch: %d/%d" % (epoch + 1, EPOCHS))\n',
                "    kbar = pkbar.Kbar(target=len(train_loader), width=100)\n",
                "\n",
                "    model.train()\n",
                "    epoch_loss = 0.0\n",
                "    for batch_id, batch_data in enumerate(train_loader):\n",
                "        # forward pass\n",
                "        optim.zero_grad()\n",
                "        input_ids = batch_data['input_ids'].to(device)\n",
                "        attention_mask = batch_data['attention_mask'].to(device)\n",
                "        # token_type_ids = batch['token_type_ids'].to(device)\n",
                "        labels = batch_data['labels'].to(device)\n",
                "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
                "\n",
                "        # backward pass\n",
                "        loss = outputs[0]\n",
                "        loss.backward()\n",
                "        optim.step()\n",
                "\n",
                "        # progress & logging\n",
                "        epoch_loss += loss.item()\n",
                '        kbar.update(batch_id, values=[("Training Loss", loss.item())])\n',
                '        writer.add_scalar("Running Loss", loss.item(), epoch * len(train_loader) + batch_id)\n',
                "\n",
                "    # log epoch training loss\n",
                '    writer.add_scalar("Epoch Train Loss", epoch_loss/len(train_dataset), epoch+1)\n',
                "    \n",
                "    # validation\n",
                "    val_loss = 0.0\n",
                "    model.eval()\n",
                "\n",
                "    # progress bar\n",
                '    print("\\nValidating Epoch: %d/%d" % (epoch + 1, EPOCHS))\n',
                "    kbar = pkbar.Kbar(target=len(val_loader), width=64)\n",
                "\n",
                "    with torch.no_grad():\n",
                "      for batch_id, batch_data in enumerate(val_loader):\n",
                "        input_ids = batch_data['input_ids'].to(device)\n",
                "        attention_mask = batch_data['attention_mask'].to(device)\n",
                "        # token_type_ids = batch['token_type_ids'].to(device)\n",
                "        labels = batch_data['labels'].to(device)\n",
                "\n",
                "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
                "        loss = outputs[0]\n",
                "\n",
                "        # progress & logging\n",
                "        val_loss += loss.item()\n",
                '        kbar.update(batch_id, values=[("Validation Loss", loss.item())])\n',
                "\n",
                "    # log epoch validation loss\n",
                '    writer.add_scalar("Epoch Valid Loss", val_loss/len(val_dataset), epoch+1)\n',
                "\n",
                "    if ((val_loss/len(val_dataset))<min_val_loss):\n",
                "        print(\"\\nModel Optimized! Saving Weights...\", '\\n')\n",
                "        min_val_loss = (val_loss/len(val_dataset))\n",
                "        checkpoint_epoch = epoch + 1\n",
                "        torch.save(model.classifier.state_dict(), SAVE_PATH / MODEL_NAME / (experiment_id + '.pt'))\n",
                "        COUNTER = 0\n",
                "    else:\n",
                "        COUNTER += 1\n",
                "        if COUNTER >= PATIENCE:\n",
                "          print('\\nEarly stopping!\\n')\n",
                "          break",
            ],
            "execution_count": null,
            "outputs": [],
        },
        {
            "cell_type": "markdown",
            "metadata": {"id": "Ck-8bbuwwKA7"},
            "source": ["# Prediction on Test"],
        },
        {
            "cell_type": "code",
            "metadata": {"id": "V4Y73rE2KlCf"},
            "source": [
                "# PREDICTIONS ON TEST \n",
                "preds = []\n",
                "ground_truth = []\n",
                "model.classifier.load_state_dict(torch.load(SAVE_PATH / MODEL_NAME / (experiment_id + '.pt')))\n",
                "model.eval()\n",
                "\n",
                "with torch.no_grad():\n",
                "    for batch_id, batch_data in enumerate(test_loader):\n",
                "        labels_index = []\n",
                "        input_ids = batch_data['input_ids'].to(device)\n",
                "        attention_mask = batch_data['attention_mask'].to(device)\n",
                "        # token_type_ids = batch['token_type_ids'].to(device)\n",
                "        labels = batch_data['labels'].to(device)\n",
                "      \n",
                "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)        \n",
                "        logits = outputs[1]\n",
                "        soft = softmax(logits, dim=2)\n",
                "        \n",
                "        # get predicted pos tag ids\n",
                "        top_p, top_class = soft.topk(1, dim=2)\n",
                "        tt = top_class.squeeze()\n",
                "\n",
                "        # retrieve relevant indices i.e. first subwords only\n",
                "        for a in range(len(labels)):\n",
                "          temp = []\n",
                "          for b in range(len(labels[a])):\n",
                "            if(labels[a][b] != -100):\n",
                "              temp.append(b)\n",
                "          labels_index.append(temp)\n",
                "\n",
                "        # store predictions of relevant indices\n",
                "        for i in range(len(labels_index)):\n",
                "          for j in (labels_index[i]):\n",
                "            preds.append(tt[i][j].item())\n",
                "            ground_truth.append(labels[i][j].item())",
            ],
            "execution_count": null,
            "outputs": [],
        },
        {
            "cell_type": "code",
            "metadata": {"id": "T59_kfPRercv"},
            "source": [
                "# USE PREDS LIST AND GROUND TRUTH LIST TO GET ACC & F1\n",
                "acc = accuracy_score(ground_truth, preds)\n",
                "prec, recall, f1, _ = precision_recall_fscore_support(ground_truth, preds, average='weighted')\n",
                'print("Accuracy: {}, F1: {}, Prec: {}, Recall: {}".format(acc, f1, prec, recall))',
            ],
            "execution_count": null,
            "outputs": [],
        },
        {
            "cell_type": "code",
            "metadata": {"id": "fK7MRDnVg8G-"},
            "source": [
                "# PRINT METRICS\n",
                'print("accuracy", round(acc,4))\n',
                'print("F1", round(f1,4))\n',
                'print("Precision", round(prec,4))\n',
                'print("recall", round(recall,4))',
            ],
            "execution_count": null,
            "outputs": [],
        },
        {
            "cell_type": "code",
            "metadata": {"id": "hmHuy6hwiKAN"},
            "source": [
                "# CLOSING LOGS\n",
                "writer.add_hparams(\n",
                "    {\n",
                '        "model": MODEL_NAME,\n',
                '        "layer": LAYER_NO,\n',
                '        "bs": BATCH_SIZE,\n',
                '        "n_epochs": EPOCHS,\n',
                '        "checkpoint_epoch": checkpoint_epoch,\n',
                '        "lr": LEARNING_RATE,\n',
                '        "dropout": DROPOUT,\n',
                '        "patience": PATIENCE,\n',
                '        "weight_decay": WEIGHT_DECAY,\n',
                "    },\n",
                '    {"validation_loss": min_val_loss, "test_acc": acc, "test_f1": f1,  "test_prec": prec,  "test_recall": recall},\n',
                ")\n",
                "writer.flush()",
            ],
            "execution_count": null,
            "outputs": [],
        },
        {
            "cell_type": "code",
            "metadata": {"id": "FBPscCz7BeTG"},
            "source": [
                "# VISUALIZE LOGS\n",
                "print(SAVE_PATH / MODEL_NAME)\n",
                '%tensorboard --logdir "checkpoints/ai4bharat/indic-bert"',
            ],
            "execution_count": null,
            "outputs": [],
        },
    ],
}
