{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {"id": "A1SouZXWbOPb"},
            "outputs": [],
            "source": [
                "# INSTALL REQUIREMENTS\n",
                "!pip install transformers==3.5.1\n",
                "!pip install pyconll\n",
                "!pip install pkbar\n",
                "!pip install torch==1.4.0",
            ],
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {"id": "w9s_coERT5vZ"},
            "outputs": [],
            "source": [
                "# IMPORTS\n",
                "from io import open\n",
                "import pathlib\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import re\n",
                "import os\n",
                "import random\n",
                "import time\n",
                "import pyconll\n",
                "import pkbar\n",
                "\n",
                "import torch\n",
                "from torch import nn\n",
                "from torch.nn.functional import softmax\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from torch.utils.tensorboard import SummaryWriter\n",
                "%load_ext tensorboard\n",
                "\n",
                "import transformers\n",
                "from transformers import BertForTokenClassification, AdamW, BertModel, BertConfig, AlbertForTokenClassification\n",
                "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
                "\n",
                "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
                "\n",
                "import matplotlib.pyplot as plt\n",
                "%matplotlib inline\n",
                "\n",
                "# SEEDING\n",
                "seed_val = 42\n",
                "random.seed(seed_val)\n",
                "np.random.seed(seed_val)\n",
                "torch.manual_seed(seed_val)\n",
                "torch.cuda.manual_seed_all(seed_val)\n",
                "\n",
                "torch.backends.cudnn.deterministic = True\n",
                "torch.backends.cudnn.benchmark = False\n",
                "\n",
                "os.environ['PYTHONHASHSEED'] = str(seed_val)\n",
                "\n",
                "# DEVICE\n",
                "device = ('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
                'print("using ", device)',
            ],
        },
        {
            "cell_type": "markdown",
            "metadata": {"id": "jDY6mHkYASLb"},
            "source": ["### HYPERPARAMS"],
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {"id": "AOXZYKBRAU9p"},
            "outputs": [],
            "source": [
                "# MODEL HYPERPARAMS\n",
                "LAYER_NO = 11\n",
                "DROPOUT = 0.1\n",
                "MODEL_NAME = 'ai4bharat/indic-bert'\n",
                "\n",
                "# OPTIMIZATION HYPERPARAMS\n",
                "LEARNING_RATE = 1e-2\n",
                "BATCH_SIZE = 128\n",
                "EPOCHS = 30\n",
                "PATIENCE = 2\n",
                "WEIGHT_DECAY = 1e-6",
            ],
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {"id": "tBjxFY8B6wSA"},
            "outputs": [],
            "source": [
                "# PATH\n",
                'TRAIN_PATH = "train.conllu" \n',
                'TEST_PATH = "test.conllu"\n',
                'DEV_PATH = "dev.conllu"\n',
                "\n",
                'SAVE_PATH = "checkpoints"\n',
                "if not os.path.exists(SAVE_PATH):\n",
                "    os.mkdir(SAVE_PATH)\n",
                "    os.mkdir(SAVE_PATH / MODEL_NAME)\n",
                "\n",
                'experiment_id = "{}{}{}".format("pos", "indic-bert", LAYER_NO)',
            ],
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {"id": "XuQXRNEFTOjj"},
            "outputs": [],
            "source": [
                "# GENERATE LISTS OF TEXT TOKENS AND CORRESPONDING POS TAGS \n",
                "def generate_data(dataset_path):\n",
                '  """\n',
                "  takes a conll file and returns token list and pos tag list\n",
                '  """ \n',
                "  dataset = pyconll.load_from_file(dataset_path)\n",
                "\n",
                "  token_text = []\n",
                "  token_postag = []\n",
                "\n",
                "  for sentence in dataset:\n",
                "\n",
                "    tokens = []\n",
                "    tags = []\n",
                "\n",
                "    for token in sentence:\n",
                "\n",
                "      tokens.append(token.form)\n",
                "      tags.append(token.upos)\n",
                "\n",
                "    token_text.append(tokens)\n",
                "    token_postag.append(tags)\n",
                "\n",
                "  return token_text, token_postag",
            ],
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {"id": "1T2XmnS0z7ap"},
            "outputs": [],
            "source": [
                "# EXTRACT TOKENS AND POS TAGS\n",
                "train_texts, train_tags = generate_data(TRAIN_PATH)\n",
                "val_texts, val_tags = generate_data(DEV_PATH)\n",
                "test_texts, test_tags = generate_data(TEST_PATH)",
            ],
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {"id": "CExfZXpVwEiO"},
            "outputs": [],
            "source": [
                "# CALCULATE LENGTH OF TRAIN, VAL AND TEST DATASETS\n",
                "train_len = len(train_tags)\n",
                "val_len = len(val_tags)\n",
                "test_len = len(test_tags)",
            ],
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {"id": "E61G7unu0xgJ"},
            "outputs": [],
            "source": [
                "# FIND NUMBER OF UNIQUE TAGS\n",
                "tags_total = train_tags + val_tags + test_tags\n",
                "\n",
                "# ENCODE TAGS TO INTEGERS AND VICE-VERSA\n",
                "unique_tags = set(tag for doc in tags_total for tag in doc)\n",
                "tag2id = {tag: id for id, tag in enumerate(unique_tags)}\n",
                "id2tag = {id: tag for tag, id in tag2id.items()}",
            ],
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {"id": "lGeOS944nc_X"},
            "outputs": [],
            "source": [
                "# LINEAR PROBING MODEL\n",
                "class BERTTokenClassifierProbe(AlbertForTokenClassification):\n",
                "    def __init__(self, config, LAYER_ID=12, DROPOUT=0.0):\n",
                "        super().__init__(config)\n",
                "        \n",
                "        # layer to extract output from, can be any integer in range [1,12]\n",
                "        self.layer_id = LAYER_ID\n",
                "        self.num_labels = config.num_labels\n",
                "\n",
                "        # enable outputing hidden states and freeze BertModel\n",
                "        self.bert = AutoModel.from_pretrained(MODEL_NAME, config=config)\n",
                "        for parameter in self.bert.parameters():\n",
                "            parameter.requires_grad = False\n",
                "\n",
                "        # linear probe\n",
                "        self.dropout = nn.Dropout(DROPOUT)\n",
                "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
                "\n",
                "    def forward(\n",
                "        self,\n",
                "        input_ids=None,\n",
                "        attention_mask=None,\n",
                "        token_type_ids=None,\n",
                "        position_ids=None,\n",
                "        head_mask=None,\n",
                "        inputs_embeds=None,\n",
                "        labels=None,\n",
                "        output_attentions=None,\n",
                "        output_hidden_states=None,\n",
                "        return_dict=None,\n",
                "    ):\n",
                '        """\n',
                "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
                "            Labels for computing the token classification loss.\n",
                "            Indices should be in ``[0, ..., config.num_labels - 1]``.\n",
                '        """\n',
                "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
                "        outputs = self.bert(\n",
                "            input_ids,\n",
                "            attention_mask=attention_mask,\n",
                "            token_type_ids=token_type_ids,\n",
                "            position_ids=position_ids,\n",
                "            head_mask=head_mask,\n",
                "            inputs_embeds=inputs_embeds,\n",
                "            output_attentions=output_attentions,\n",
                "            output_hidden_states=output_hidden_states,\n",
                "            return_dict=return_dict,\n",
                "        )\n",
                "        # taking output from the specified layer\n",
                "        sequence_output = outputs[2][self.layer_id]\n",
                "        sequence_output = self.dropout(sequence_output)\n",
                "        logits = self.classifier(sequence_output)\n",
                "        loss = None\n",
                "        if labels is not None:\n",
                "            loss_fct = nn.CrossEntropyLoss()\n",
                "            # Only keep active parts of the loss\n",
                "            if attention_mask is not None:\n",
                "                active_loss = attention_mask.view(-1) == 1\n",
                "                active_logits = logits.view(-1, self.num_labels)\n",
                "                active_labels = torch.where(\n",
                "                    active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n",
                "                )\n",
                "                loss = loss_fct(active_logits, active_labels)\n",
                "            else:\n",
                "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
                "        if not return_dict:\n",
                "            output = (logits,) + outputs[2:]\n",
                "            return ((loss,) + output) if loss is not None else output\n",
                "        return TokenClassifierOutput(\n",
                "            loss=loss,\n",
                "            logits=logits,\n",
                "            hidden_states=outputs.hidden_states,\n",
                "            attentions=outputs.attentions,\n",
                "        )",
            ],
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {"id": "wS6_x5TMNfLL"},
            "outputs": [],
            "source": [
                "# TOKENIZER\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast = True)\n",
                "\n",
                "# PRE-TRAINED MODEL and PROBE\n",
                "config = AutoConfig.from_pretrained(MODEL_NAME, output_hidden_states=True, num_labels=len(unique_tags))\n",
                "model = BERTTokenClassifierProbe(config, LAYER_ID=LAYER_NO, DROPOUT=DROPOUT).to(device)",
            ],
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {"id": "J_Zsr9IDGzyJ"},
            "outputs": [],
            "source": [
                "# TOKENIZE DATA\n",
                "# adding max_length = 128 for Indic-Bert(wasn't reqd in mBERT)\n",
                "train_encodings = tokenizer(train_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True, max_length = 128)\n",
                "val_encodings = tokenizer(val_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True, max_length = 128)\n",
                "test_encodings = tokenizer(test_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True, max_length = 128)",
            ],
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {"id": "w3UHtFJxv9YY"},
            "outputs": [],
            "source": [
                "# CONVERT THE TOKENS TO THEIR RESPECTIVE INTEGER IDS\n",
                "def convert_token_to_id(encodings, length):\n",
                "  token_to_id = []\n",
                "  for i in range(length):\n",
                "    token_to_id.append(encodings[i].tokens)\n",
                "  return token_to_id\n",
                "\n",
                "train_token_ids = convert_token_to_id(train_encodings, train_len)\n",
                "val_token_ids = convert_token_to_id(val_encodings, val_len)\n",
                "test_token_ids = convert_token_to_id(test_encodings, test_len)",
            ],
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {"id": "CxQsPKTYKK8R"},
            "outputs": [],
            "source": [
                "# ENCODE THE TAGS IN A SPECIFIC MANNER\n",
                "def encode_tags(tags, encodings, tokenizer, token_id):\n",
                '  """\n',
                "  takes tokenizer encodings as input, \n",
                '  """\n',
                "  labels = [[tag2id[tag] for tag in doc] for doc in tags] # convert pos tags to their integer ID\n",
                "  encoded_labels = []\n",
                "  labels_index = []\n",
                "  for doc_labels, doc_offset, doc_token_id in zip(labels, encodings.offset_mapping, token_id):\n",
                "    # create an empty array of -100\n",
                "    doc_enc_labels = np.ones(len(doc_offset), dtype=int) * -100 # array of length=number of word piece tokens in sentence\n",
                "    arr_offset = np.array(doc_offset) # convert doc_offset to numpy array\n",
                "    \n",
                "    # set labels whose first offset position is 0 (all other sub-words other than initial one) and the second is not 0 (CLS and PAD token)\n",
                "    a = (arr_offset[:,0] == 0) & (arr_offset[:,1] != 0) # true at required labels and false were the labels need to be ignored\n",
                "    \n",
                "    # print(doc_labels)\n",
                "    # print(doc_offset)\n",
                "    li = []\n",
                "    countl = 0 # count of doc_labels (original labels)\n",
                "    countel = 0 # count of doc_enc_labels (encoded labels) \n",
                "    for j in a: # iterate through every element of 'a'\n",
                "      if j:\n",
                "        if tokenizer.convert_tokens_to_ids(doc_token_id[countel]) != 8: # ignore the extra '_' token generated due to indicbert tokenizer\n",
                "          li.append(countel)\n",
                "          doc_enc_labels[countel] = doc_labels[countl] # if true append the label from doc_labels to the new required encoded labels list\n",
                "          countl = countl + 1 \n",
                "        countel = countel + 1 \n",
                "      else: # if false skip that label from doc_label and move to the next index at doc_enc_labels\n",
                "        countel = countel + 1\n",
                "\n",
                "    encoded_labels.append(doc_enc_labels.tolist())\n",
                "    labels_index.append(li)\n",
                "    \n",
                "  return encoded_labels, labels_index\n",
                "\n",
                "train_labels, train_labels_index = encode_tags(train_tags, train_encodings, tokenizer, train_token_ids)\n",
                "val_labels, val_labels_index = encode_tags(val_tags, val_encodings, tokenizer, val_token_ids)\n",
                "test_labels, test_labels_index = encode_tags(test_tags, test_encodings, tokenizer, test_token_ids)",
            ],
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {"id": "g3PgrqeOj7nX"},
            "outputs": [],
            "source": [
                "# DATASETS\n",
                "class POSdataset(Dataset):\n",
                "    def __init__(self, encodings, labels):\n",
                "        self.encodings = encodings\n",
                "        self.labels = labels\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
                "        item['labels'] = torch.tensor(self.labels[idx])\n",
                "        return item\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.labels)\n",
                " \n",
                "train_dataset = POSdataset(train_encodings, train_labels)\n",
                "val_dataset = POSdataset(val_encodings, val_labels)\n",
                "test_dataset = POSdataset(test_encodings, test_labels)",
            ],
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {"id": "1bWAinOtz8KK"},
            "outputs": [],
            "source": [
                "# DATALOADERS\n",
                "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
                "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
                "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
                "\n",
                "optim = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)",
            ],
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {"id": "NbRl0CcbhAl4"},
            "outputs": [],
            "source": [
                "# TENSORBOARD LOGGER\n",
                "writer = SummaryWriter(log_dir=str(SAVE_PATH / MODEL_NAME), comment=experiment_id)",
            ],
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {"colab": {"background_save": true}, "id": "Z_3gdSZMtiLY"},
            "outputs": [],
            "source": [
                "# TRAIN PROBE\n",
                "min_val_loss = 999999\n",
                "checkpoint_epoch = 0\n",
                "COUNTER = 0\n",
                "\n",
                "for epoch in range(EPOCHS):\n",
                "\n",
                "    # progress bar\n",
                '    print("\\nTraining Epoch: %d/%d" % (epoch + 1, EPOCHS))\n',
                "    kbar = pkbar.Kbar(target=len(train_loader), width=100)\n",
                "\n",
                "    model.train()\n",
                "    epoch_loss = 0.0\n",
                "    for batch_id, batch_data in enumerate(train_loader):\n",
                "        # forward pass\n",
                "        optim.zero_grad()\n",
                "        input_ids = batch_data['input_ids'].to(device)\n",
                "        attention_mask = batch_data['attention_mask'].to(device)\n",
                "        # token_type_ids = batch['token_type_ids'].to(device)\n",
                "        labels = batch_data['labels'].to(device)\n",
                "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
                "\n",
                "        # backward pass\n",
                "        loss = outputs[0]\n",
                "        loss.backward()\n",
                "        optim.step()\n",
                "\n",
                "        # progress & logging\n",
                "        epoch_loss += loss.item()\n",
                '        kbar.update(batch_id, values=[("Training Loss", loss.item())])\n',
                '        writer.add_scalar("Running Loss", loss.item(), epoch * len(train_loader) + batch_id)\n',
                "\n",
                "    # log epoch training loss\n",
                '    writer.add_scalar("Epoch Train Loss", epoch_loss/len(train_dataset), epoch+1)\n',
                "    \n",
                "    # validation\n",
                "    val_loss = 0.0\n",
                "    model.eval()\n",
                "\n",
                "    # progress bar\n",
                '    print("\\nValidating Epoch: %d/%d" % (epoch + 1, EPOCHS))\n',
                "    kbar = pkbar.Kbar(target=len(val_loader), width=64)\n",
                "\n",
                "    with torch.no_grad():\n",
                "      for batch_id, batch_data in enumerate(val_loader):\n",
                "        input_ids = batch_data['input_ids'].to(device)\n",
                "        attention_mask = batch_data['attention_mask'].to(device)\n",
                "        # token_type_ids = batch['token_type_ids'].to(device)\n",
                "        labels = batch_data['labels'].to(device)\n",
                "\n",
                "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
                "        loss = outputs[0]\n",
                "\n",
                "        # progress & logging\n",
                "        val_loss += loss.item()\n",
                '        kbar.update(batch_id, values=[("Validation Loss", loss.item())])\n',
                "\n",
                "    # log epoch validation loss\n",
                '    writer.add_scalar("Epoch Valid Loss", val_loss/len(val_dataset), epoch+1)\n',
                "\n",
                "    if ((val_loss/len(val_dataset))<min_val_loss):\n",
                "        print(\"\\nModel Optimized! Saving Weights...\", '\\n')\n",
                "        min_val_loss = (val_loss/len(val_dataset))\n",
                "        checkpoint_epoch = epoch + 1\n",
                "        torch.save(model.classifier.state_dict(), SAVE_PATH / MODEL_NAME / (experiment_id + '.pt'))\n",
                "        COUNTER = 0\n",
                "    else:\n",
                "        COUNTER += 1\n",
                "        if COUNTER >= PATIENCE:\n",
                "          print('\\nEarly stopping!\\n')\n",
                "          break",
            ],
        },
        {
            "cell_type": "markdown",
            "metadata": {"id": "Ck-8bbuwwKA7"},
            "source": ["# Prediction on Test"],
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {"id": "V4Y73rE2KlCf"},
            "outputs": [],
            "source": [
                "# PREDICTIONS ON TEST \n",
                "preds = []\n",
                "ground_truth = []\n",
                "model.classifier.load_state_dict(torch.load(SAVE_PATH / MODEL_NAME / (experiment_id + '.pt')))\n",
                "model.eval()\n",
                "\n",
                "with torch.no_grad():\n",
                "    for batch_id, batch_data in enumerate(test_loader):\n",
                "        labels_index = []\n",
                "        input_ids = batch_data['input_ids'].to(device)\n",
                "        attention_mask = batch_data['attention_mask'].to(device)\n",
                "        # token_type_ids = batch['token_type_ids'].to(device)\n",
                "        labels = batch_data['labels'].to(device)\n",
                "      \n",
                "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)        \n",
                "        logits = outputs[1]\n",
                "        soft = softmax(logits, dim=2)\n",
                "        \n",
                "        # get predicted pos tag ids\n",
                "        top_p, top_class = soft.topk(1, dim=2)\n",
                "        tt = top_class.squeeze()\n",
                "\n",
                "        # retrieve relevant indices i.e. first subwords only\n",
                "        for a in range(len(labels)):\n",
                "          temp = []\n",
                "          for b in range(len(labels[a])):\n",
                "            if(labels[a][b] != -100):\n",
                "              temp.append(b)\n",
                "          labels_index.append(temp)\n",
                "\n",
                "        # store predictions of relevant indices\n",
                "        for i in range(len(labels_index)):\n",
                "          for j in (labels_index[i]):\n",
                "            preds.append(tt[i][j].item())\n",
                "            ground_truth.append(labels[i][j].item())",
            ],
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {"id": "T59_kfPRercv"},
            "outputs": [],
            "source": [
                "# USE PREDS LIST AND GROUND TRUTH LIST TO GET ACC & F1\n",
                "acc = accuracy_score(ground_truth, preds)\n",
                "prec, recall, f1, _ = precision_recall_fscore_support(ground_truth, preds, average='weighted')\n",
                'print("Accuracy: {}, F1: {}, Prec: {}, Recall: {}".format(acc, f1, prec, recall))',
            ],
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {"id": "C-XFcMlexe8w"},
            "outputs": [],
            "source": [
                "# PRINT METRICS\n",
                'print("accuracy", round(acc,4))\n',
                'print("F1", round(f1,4))\n',
                'print("Precision", round(prec,4))\n',
                'print("recall", round(recall,4))',
            ],
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {"id": "hmHuy6hwiKAN"},
            "outputs": [],
            "source": [
                "# CLOSING LOGS\n",
                "writer.add_hparams(\n",
                "    {\n",
                '        "model": MODEL_NAME,\n',
                '        "layer": LAYER_NO,\n',
                '        "bs": BATCH_SIZE,\n',
                '        "n_epochs": EPOCHS,\n',
                '        "checkpoint_epoch": checkpoint_epoch,\n',
                '        "lr": LEARNING_RATE,\n',
                '        "dropout": DROPOUT,\n',
                '        "patience": PATIENCE,\n',
                '        "weight_decay": WEIGHT_DECAY,\n',
                "    },\n",
                '    {"validation_loss": min_val_loss, "test_acc": acc, "test_f1": f1,  "test_prec": prec,  "test_recall": recall},\n',
                ")\n",
                "writer.flush()",
            ],
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {"id": "FBPscCz7BeTG"},
            "outputs": [],
            "source": [
                "# VISUALIZE LOGS\n",
                "print(SAVE_PATH / MODEL_NAME)\n",
                '%tensorboard --logdir "checkpoints/ai4bharat/indic-bert"',
            ],
        },
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "collapsed_sections": [],
            "machine_shape": "hm",
            "name": "PoS Tagging.ipynb",
            "provenance": [],
        },
        "kernelspec": {"display_name": "Python 3", "name": "python3"},
    },
    "nbformat": 4,
    "nbformat_minor": 0,
}
