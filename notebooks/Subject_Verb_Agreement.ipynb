{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {"id": "A1SouZXWbOPb"},
            "outputs": [],
            "source": [
                "# INSTALL REQUIREMENTS\n",
                "!pip install transformers==3.5.1\n",
                "!pip install pyconll\n",
                "!pip install conllu\n",
                "!pip install pkbar\n",
                "!pip install torch==1.4.0",
            ],
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {"id": "w9s_coERT5vZ"},
            "outputs": [],
            "source": [
                "# IMPORTS\n",
                "from io import open\n",
                "import pathlib\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import re\n",
                "import os\n",
                "import random\n",
                "import time\n",
                "import pyconll\n",
                "import conllu\n",
                "from conllu import parse_incr\n",
                "import pkbar\n",
                "\n",
                "import torch\n",
                "from torch import nn\n",
                "from torch.nn.functional import softmax\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from torch.utils.tensorboard import SummaryWriter\n",
                "%load_ext tensorboard\n",
                "\n",
                "import transformers\n",
                "from transformers import BertForSequenceClassification, AdamW, BertModel, BertConfig, AlbertForSequenceClassification\n",
                "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
                "\n",
                "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
                "\n",
                "import matplotlib.pyplot as plt\n",
                "%matplotlib inline\n",
                "\n",
                "# SEEDING\n",
                "seed_val = 42\n",
                "random.seed(seed_val)\n",
                "np.random.seed(seed_val)\n",
                "torch.manual_seed(seed_val)\n",
                "torch.cuda.manual_seed_all(seed_val)\n",
                "\n",
                "torch.backends.cudnn.deterministic = True\n",
                "torch.backends.cudnn.benchmark = False\n",
                "\n",
                "os.environ['PYTHONHASHSEED'] = str(seed_val)\n",
                "\n",
                "# DEVICE\n",
                "device = ('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
                'print("using ", device)',
            ],
        },
        {
            "cell_type": "markdown",
            "metadata": {"id": "jDY6mHkYASLb"},
            "source": ["### HYPERPARAMS"],
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {"id": "AOXZYKBRAU9p"},
            "outputs": [],
            "source": [
                "# MODEL HYPERPARAMS\n",
                "LAYER_NO = 10\n",
                "DROPOUT = 0.1\n",
                "MODEL_NAME = 'ai4bharat/indic-bert'\n",
                "\n",
                "# OPTIMIZATION HYPERPARAMS\n",
                "LEARNING_RATE = 1e-2\n",
                "BATCH_SIZE = 128\n",
                "EPOCHS = 30\n",
                "PATIENCE = 2\n",
                "WEIGHT_DECAY = 1e-6",
            ],
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {"id": "oFyRELT9C0YA"},
            "outputs": [],
            "source": [
                "# TOKENIZER\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)",
            ],
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {"id": "tBjxFY8B6wSA"},
            "outputs": [],
            "source": [
                "# PATH\n",
                'TRAIN_PATH = "train.conllu" \n',
                'TEST_PATH = "test.conllu"\n',
                'DEV_PATH = "dev.conllu"\n',
                "\n",
                'SAVE_PATH = "checkpoints"\n',
                "if not os.path.exists(SAVE_PATH):\n",
                "    os.mkdir(SAVE_PATH)\n",
                "    os.mkdir(SAVE_PATH / MODEL_NAME)\n",
                "\n",
                'experiment_id = "{}_{}_{}".format("num-gender-agree", "indic-bert", LAYER_NO)',
            ],
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {"id": "J02xHq9Us25s"},
            "outputs": [],
            "source": [
                "# GENERATE COMPLETE SET OF SENTENCES\n",
                "def generate_sentences(dataset_path):\n",
                "  dataset = pyconll.load_from_file(dataset_path)\n",
                "\n",
                "  sentences = []\n",
                "\n",
                "  for s in dataset:\n",
                "    inter = []\n",
                "    for t in s:\n",
                "      inter.append(t.form) \n",
                '    sentences.append(" ".join(inter))\n',
                "\n",
                "  return sentences\n",
                "\n",
                "train_sentences = generate_sentences(TRAIN_PATH)\n",
                "val_sentences = generate_sentences(DEV_PATH)\n",
                "test_sentences = generate_sentences(TEST_PATH)\n",
                "\n",
                "print(len(train_sentences))\n",
                "print(len(val_sentences))\n",
                "print(len(test_sentences))",
            ],
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {"id": "TPYdlUO5iZ-0"},
            "outputs": [],
            "source": [
                "# GENERATE LISTS OF SENTENCES AND CORRESPONDING GENDER-NUMBER AGREEMENTS\n",
                "def generate_data(dataset_path, total_sentences):\n",
                "  #returns a sentence the verbs number\n",
                "  dataset = pyconll.load_from_file(dataset_path)\n",
                "\n",
                "  sentences = []\n",
                "  number_gender = []\n",
                "  verb_index = []\n",
                "  # sentence_index = []\n",
                "\n",
                "  sent_index = 0\n",
                "\n",
                "  for sentence in dataset:\n",
                "   \n",
                "    # inter = []\n",
                "    skip = False\n",
                "    verb_present = False\n",
                "    token_index = 0\n",
                "\n",
                "    for token in sentence:\n",
                "\n",
                "      if token.upos == 'VERB':\n",
                "        verb_present = True\n",
                "        try:\n",
                "          num = str(token.feats['Number'])\n",
                "          gender = str(token.feats['Gender'])\n",
                "          number_gender.append(num + gender)\n",
                "          verb_index.append(token_index)\n",
                "          # sentence_index.append(sent_index)\n",
                "          sentences.append(total_sentences[sent_index])\n",
                "        except:\n",
                "          skip = True        \n",
                "      \n",
                "      token_index+=1\n",
                "    \n",
                "    sent_index+=1\n",
                "\n",
                "  return sentences, number_gender, verb_index",
            ],
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {"id": "1T2XmnS0z7ap"},
            "outputs": [],
            "source": [
                "# EXTRACT SENTENCE, DEPTH AND VERB INDEX\n",
                "train_texts, train_number, train_verb_index = generate_data(TRAIN_PATH, train_sentences)\n",
                "val_texts, val_number, val_verb_index = generate_data(DEV_PATH, val_sentences)\n",
                "test_texts, test_number, test_verb_index = generate_data(TEST_PATH, test_sentences)",
            ],
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {"id": "xSxs_jmNAIzf"},
            "outputs": [],
            "source": [
                "# MASK THE POSITION OF THE VERB\n",
                "def masked_text(sentences, verb_index, tokenizer):\n",
                "\n",
                "  masked_texts = []\n",
                "  for sent, index in zip(sentences, verb_index):\n",
                "    a = sent.split()\n",
                "    a[index] = tokenizer.mask_token\n",
                '    b = " ".join(a)\n',
                "    masked_texts.append(b)\n",
                "\n",
                "  return masked_texts",
            ],
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {"id": "--EfTkGoCI8-"},
            "outputs": [],
            "source": [
                "# EXTRACT MASKED SENTENCES FOR TRAIN, VAL AND TEST\n",
                "masked_train_texts = masked_text(train_texts, train_verb_index, tokenizer)\n",
                "masked_val_texts = masked_text(val_texts, val_verb_index, tokenizer)\n",
                "masked_test_texts = masked_text(test_texts, test_verb_index, tokenizer)",
            ],
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {"id": "-SA1wEBQ3N0S"},
            "outputs": [],
            "source": [
                "# GET THE UNIQUE NUMBERS AVAILABLE IN THE TOTAL DATASET\n",
                "total_number = train_number + val_number + test_number\n",
                "total_number_unique = set(total_number)\n",
                "unique_number = len(total_number_unique)",
            ],
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {"id": "O5xBcwoGP0-d"},
            "outputs": [],
            "source": [
                "# CONVERT TO NUMERIC LABELS\n",
                "def categorical_to_numeric(numbers):\n",
                "\n",
                "  labels = []\n",
                "  for i in numbers:\n",
                "    if i == \"{'Sing'}{'Masc'}\":\n",
                "      labels.append(0)\n",
                "    elif i == \"{'Sing'}{'Fem'}\":\n",
                "      labels.append(1)\n",
                "    elif i == \"{'Plur'}{'Masc'}\":\n",
                "      labels.append(2)\n",
                "    else:\n",
                "      labels.append(3)\n",
                "      \n",
                "  return labels",
            ],
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {"id": "M0BDSDS9RtxB"},
            "outputs": [],
            "source": [
                "# EXTRACT THE NUMERIC LABELS\n",
                "train_labels = categorical_to_numeric(train_number)\n",
                "val_labels = categorical_to_numeric(val_number)\n",
                "test_labels = categorical_to_numeric(test_number)",
            ],
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {"id": "KxtuJ_ist4jg"},
            "outputs": [],
            "source": [
                "# LINEAR PROBING MODEL\n",
                "class SequenceClassifierProbe(AlbertForSequenceClassification):\n",
                "    def __init__(self, config, LAYER_ID=12, DROPOUT=0.0):\n",
                "        super().__init__(config)\n",
                "        \n",
                "        # layer to extract output from, can be any integer in range [1,12]\n",
                "        self.layer_id = LAYER_ID\n",
                "        self.num_labels = config.num_labels\n",
                "\n",
                "        # enable outputing hidden states and freeze BertModel\n",
                "        self.lm = AutoModel.from_pretrained(MODEL_NAME, config=config)\n",
                "        for parameter in self.lm.parameters():\n",
                "            parameter.requires_grad = False\n",
                "\n",
                "        # linear probe\n",
                "        self.dropout = nn.Dropout(DROPOUT)\n",
                "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
                "\n",
                "    def forward(\n",
                "        self,\n",
                "        input_ids=None,\n",
                "        attention_mask=None,\n",
                "        token_type_ids=None,\n",
                "        position_ids=None,\n",
                "        head_mask=None,\n",
                "        inputs_embeds=None,\n",
                "        labels=None,\n",
                "        output_attentions=None,\n",
                "        output_hidden_states=None,\n",
                "        return_dict=None,\n",
                "    ):\n",
                '        """\n',
                "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
                "            Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[0, ...,\n",
                "            config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n",
                "            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
                '        """\n',
                "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
                "        outputs = self.lm(\n",
                "            input_ids,\n",
                "            attention_mask=attention_mask,\n",
                "            token_type_ids=token_type_ids,\n",
                "            position_ids=position_ids,\n",
                "            head_mask=head_mask,\n",
                "            inputs_embeds=inputs_embeds,\n",
                "            output_attentions=output_attentions,\n",
                "            output_hidden_states=output_hidden_states,\n",
                "            return_dict=return_dict,\n",
                "        )\n",
                "                \n",
                "\n",
                "        # sequence_output = outputs[0]\n",
                "        # pooled_output = outputs[1]\n",
                "        hidden_states = outputs[2]\n",
                "        # -1 for 12th layer, -2 for 11th layer and so on\n",
                "        #h = hidden_states[layer_id - 13][:, 0].reshape((-1, 1, 768)) \n",
                "        h = torch.squeeze(hidden_states[self.layer_id][:, 0, :])\n",
                "        pooled_output = self.dropout(h)\n",
                "        logits = self.classifier(pooled_output)\n",
                "        loss = None\n",
                "\n",
                "        if labels is not None:\n",
                "            if self.num_labels == 1:\n",
                "                #  We are doing regression\n",
                "                loss_fct = MSELoss()\n",
                "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
                "            else:\n",
                "                loss_fct = nn.CrossEntropyLoss()\n",
                "                # print(logits.shape)\n",
                "                a = logits.view(-1, self.num_labels)\n",
                "                # print(a.shape)\n",
                "                b = labels.view(-1)\n",
                "                # print(b.shape)\n",
                "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
                "\n",
                "        if not return_dict:\n",
                "            output = (logits,) + outputs[2:]\n",
                "            return ((loss,) + output) if loss is not None else output\n",
                "\n",
                "        return SequenceClassifierOutput(\n",
                "            loss=loss,\n",
                "            logits=logits,\n",
                "            hidden_states=outputs.hidden_states,\n",
                "            attentions=outputs.attentions,\n",
                "        )",
            ],
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {"id": "U2TXU2Ubyh7T"},
            "outputs": [],
            "source": [
                "# PRE-TRAINED MODEL and PROBE\n",
                "config = AutoConfig.from_pretrained(MODEL_NAME, output_hidden_states=True, num_labels=unique_number, max_position_embeddings=512)\n",
                "model = SequenceClassifierProbe(config, LAYER_ID=LAYER_NO, DROPOUT=DROPOUT).to(device)",
            ],
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {"id": "0jbW77MK6MN8"},
            "outputs": [],
            "source": [
                "# TOKENIZE DATA\n",
                "train_encodings = tokenizer(masked_train_texts, truncation=True, padding=True)\n",
                "val_encodings = tokenizer(masked_val_texts, truncation=True, padding=True)\n",
                "test_encodings = tokenizer(masked_test_texts, truncation=True, padding=True)",
            ],
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {"id": "La4UhcGH7-sh"},
            "outputs": [],
            "source": [
                "# DATASETS\n",
                "class NumberAgreementDataset(Dataset):\n",
                "    def __init__(self, encodings, labels):\n",
                "        self.encodings = encodings\n",
                "        self.labels = labels\n",
                "        \n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
                "        item['labels'] = torch.tensor(self.labels[idx])\n",
                "        return item\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.labels)\n",
                "\n",
                "train_dataset = NumberAgreementDataset(train_encodings, train_labels)\n",
                "val_dataset = NumberAgreementDataset(val_encodings, val_labels)\n",
                "test_dataset = NumberAgreementDataset(test_encodings, test_labels)",
            ],
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {"id": "4DQjfO2V7plN"},
            "outputs": [],
            "source": [
                "# DATALOADERS\n",
                "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
                "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
                "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
                "\n",
                "optim = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)",
            ],
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {"id": "V2siXTqUVB_r"},
            "outputs": [],
            "source": [
                "# TENSORBOARD LOGGER\n",
                "writer = SummaryWriter(log_dir=str(SAVE_PATH / MODEL_NAME), comment=experiment_id)",
            ],
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {"colab": {"background_save": true}, "id": "QEP9EYLzVC5N"},
            "outputs": [],
            "source": [
                "# TRAIN PROBE\n",
                "min_val_loss = 999999\n",
                "checkpoint_epoch = 0\n",
                "COUNTER = 0\n",
                "\n",
                "for epoch in range(EPOCHS):\n",
                "\n",
                "    # progress bar\n",
                '    print("\\nTraining Epoch: %d/%d" % (epoch + 1, EPOCHS))\n',
                "    kbar = pkbar.Kbar(target=len(train_loader), width=100)\n",
                "\n",
                "    model.train()\n",
                "    epoch_loss = 0.0\n",
                "    for batch_id, batch_data in enumerate(train_loader):\n",
                "        # forward pass\n",
                "        optim.zero_grad()\n",
                "        input_ids = batch_data['input_ids'].to(device)\n",
                "        attention_mask = batch_data['attention_mask'].to(device)\n",
                "        # token_type_ids = batch['token_type_ids'].to(device)\n",
                "        labels = batch_data['labels'].to(device)\n",
                "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
                "\n",
                "        # backward pass\n",
                "        loss = outputs[0]\n",
                "        loss.backward()\n",
                "        optim.step()\n",
                "\n",
                "        # progress & logging\n",
                "        epoch_loss += loss.item()\n",
                '        kbar.update(batch_id, values=[("Training Loss", loss.item())])\n',
                '        writer.add_scalar("Running Loss", loss.item(), epoch * len(train_loader) + batch_id)\n',
                "\n",
                "    # log epoch training loss\n",
                '    writer.add_scalar("Epoch Train Loss", epoch_loss/len(train_dataset), epoch+1)\n',
                "    \n",
                "    # validation\n",
                "    val_loss = 0.0\n",
                "    model.eval()\n",
                "\n",
                "    # progress bar\n",
                '    print("\\nValidating Epoch: %d/%d" % (epoch + 1, EPOCHS))\n',
                "    kbar = pkbar.Kbar(target=len(val_loader), width=64)\n",
                "\n",
                "    with torch.no_grad():\n",
                "      for batch_id, batch_data in enumerate(val_loader):\n",
                "        input_ids = batch_data['input_ids'].to(device)\n",
                "        attention_mask = batch_data['attention_mask'].to(device)\n",
                "        # token_type_ids = batch['token_type_ids'].to(device)\n",
                "        labels = batch_data['labels'].to(device)\n",
                "\n",
                "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
                "        loss = outputs[0]\n",
                "\n",
                "        # progress & logging\n",
                "        val_loss += loss.item()\n",
                '        kbar.update(batch_id, values=[("Validation Loss", loss.item())])\n',
                "\n",
                "    # log epoch validation loss\n",
                '    writer.add_scalar("Epoch Valid Loss", val_loss/len(val_dataset), epoch+1)\n',
                "\n",
                "    if ((val_loss/len(val_dataset))<min_val_loss):\n",
                "        print(\"\\nModel Optimized! Saving Weights...\", '\\n')\n",
                "        min_val_loss = (val_loss/len(val_dataset))\n",
                "        checkpoint_epoch = epoch + 1\n",
                "        torch.save(model.classifier.state_dict(), SAVE_PATH / MODEL_NAME / (experiment_id + '.pt'))\n",
                "        COUNTER = 0\n",
                "    else:\n",
                "        COUNTER += 1\n",
                "        if COUNTER >= PATIENCE:\n",
                "          print('\\nEarly stopping!\\n')\n",
                "          break",
            ],
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {"id": "HeIJ71OlVHAo"},
            "outputs": [],
            "source": [
                "# PREDICTIONS ON TEST \n",
                "preds = []\n",
                "truth = []\n",
                "model.classifier.load_state_dict(torch.load(SAVE_PATH / MODEL_NAME / (experiment_id + '.pt')))\n",
                "model.eval()\n",
                "\n",
                "with torch.no_grad():\n",
                "    for batch_id, batch_data in enumerate(test_loader):\n",
                "        labels_index = []\n",
                "        input_ids = batch_data['input_ids'].to(device)\n",
                "        attention_mask = batch_data['attention_mask'].to(device)\n",
                "        # token_type_ids = batch['token_type_ids'].to(device)\n",
                "        labels = batch_data['labels'].to(device)\n",
                "      \n",
                "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)        \n",
                "        logits = outputs[1]\n",
                "        soft = softmax(logits, dim=1)\n",
                "        _, predictions = torch.max(soft, dim=1, keepdim=False)\n",
                "        for i in range(len(predictions)):\n",
                "          preds.append(predictions[i].item())\n",
                "        for label in labels:\n",
                "          truth.append(label.item())",
            ],
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {"colab": {"background_save": true}, "id": "FYCVgZrlVLdm"},
            "outputs": [],
            "source": [
                "# USE PREDS LIST AND GROUND TRUTH LIST TO GET ACC & F1\n",
                "acc = accuracy_score(truth, preds)\n",
                "prec, recall, f1, _ = precision_recall_fscore_support(truth, preds, average='weighted')\n",
                'print("Accuracy: {}, F1: {}, Prec: {}, Recall: {}".format(acc, f1, prec, recall))',
            ],
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {"id": "EliLbsoQVOkK"},
            "outputs": [],
            "source": [
                "# PRINT METRICS\n",
                'print("accuracy", round(acc,4))\n',
                'print("F1", round(f1,4))\n',
                'print("Precision", round(prec,4))\n',
                'print("recall", round(recall,4))',
            ],
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {"id": "CAX4PI1cVSEO"},
            "outputs": [],
            "source": [
                "# CLOSING LOGS\n",
                "writer.add_hparams(\n",
                "    {\n",
                '        "model": MODEL_NAME,\n',
                '        "layer": LAYER_NO,\n',
                '        "bs": BATCH_SIZE,\n',
                '        "n_epochs": EPOCHS,\n',
                '        "checkpoint_epoch": checkpoint_epoch,\n',
                '        "lr": LEARNING_RATE,\n',
                '        "dropout": DROPOUT,\n',
                '        "patience": PATIENCE,\n',
                '        "weight_decay": WEIGHT_DECAY,\n',
                "    },\n",
                '    {"validation_loss": min_val_loss, "test_acc": acc, "test_f1": f1,  "test_prec": prec,  "test_recall": recall},\n',
                ")\n",
                "writer.flush()",
            ],
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {"id": "hOgFaRRqVYMN"},
            "outputs": [],
            "source": [
                "# VISUALIZE LOGS\n",
                "print(SAVE_PATH / MODEL_NAME)\n",
                '%tensorboard --logdir "checkpoints/ai4bharat/indic-bert"',
            ],
        },
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "collapsed_sections": [],
            "machine_shape": "hm",
            "name": "Subject-Verb Agreement.ipynb",
            "provenance": [],
        },
        "kernelspec": {"display_name": "Python 3", "name": "python3"},
    },
    "nbformat": 4,
    "nbformat_minor": 0,
}
